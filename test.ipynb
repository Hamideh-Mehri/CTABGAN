{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 20:22:35.284711: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "from data_transformer import DataTransformer\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata = pd.read_csv('adult.csv')\n",
    "cat_cols = [ 'workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'gender', 'native-country', 'income']\n",
    "log_cols = []\n",
    "mixed_cols= {'capital-loss':[0.0],'capital-gain':[0.0]}\n",
    "num_cols = ['age', 'fnlwgt','educational-num','hours-per-week']\n",
    "target_col = 'income'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = DataTransformer(rawdata, cat_cols, num_cols,mixed_cols, log_cols, target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess data\n",
    "df = transformer.transformData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37886</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>100829</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24949</th>\n",
       "      <td>27</td>\n",
       "      <td>Private</td>\n",
       "      <td>81540</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Craft-repair</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24629</th>\n",
       "      <td>51</td>\n",
       "      <td>Private</td>\n",
       "      <td>123703</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>4386</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25783</th>\n",
       "      <td>33</td>\n",
       "      <td>Private</td>\n",
       "      <td>255334</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34688</th>\n",
       "      <td>32</td>\n",
       "      <td>Private</td>\n",
       "      <td>257043</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16040</th>\n",
       "      <td>40</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>36296</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5882</th>\n",
       "      <td>27</td>\n",
       "      <td>Private</td>\n",
       "      <td>135296</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20602</th>\n",
       "      <td>32</td>\n",
       "      <td>Private</td>\n",
       "      <td>80058</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42656</th>\n",
       "      <td>30</td>\n",
       "      <td>?</td>\n",
       "      <td>201196</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>?</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>35</td>\n",
       "      <td>Federal-gov</td>\n",
       "      <td>413930</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Craft-repair</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43957 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age    workclass  fnlwgt  education  educational-num  \\\n",
       "37886   28      Private  100829  Bachelors               13   \n",
       "24949   27      Private   81540    HS-grad                9   \n",
       "24629   51      Private  123703    HS-grad                9   \n",
       "25783   33      Private  255334  Bachelors               13   \n",
       "34688   32      Private  257043    HS-grad                9   \n",
       "...    ...          ...     ...        ...              ...   \n",
       "16040   40    Local-gov   36296  Bachelors               13   \n",
       "5882    27      Private  135296  Bachelors               13   \n",
       "20602   32      Private   80058    HS-grad                9   \n",
       "42656   30            ?  201196       11th                7   \n",
       "280     35  Federal-gov  413930    HS-grad                9   \n",
       "\n",
       "           marital-status         occupation   relationship   race  gender  \\\n",
       "37886  Married-civ-spouse    Exec-managerial           Wife  White  Female   \n",
       "24949  Married-civ-spouse       Craft-repair        Husband  White    Male   \n",
       "24629  Married-civ-spouse  Machine-op-inspct        Husband  White    Male   \n",
       "25783  Married-civ-spouse     Prof-specialty        Husband  White    Male   \n",
       "34688       Never-married       Adm-clerical  Not-in-family  White  Female   \n",
       "...                   ...                ...            ...    ...     ...   \n",
       "16040  Married-civ-spouse     Prof-specialty        Husband  White    Male   \n",
       "5882   Married-civ-spouse    Exec-managerial           Wife  White  Female   \n",
       "20602  Married-civ-spouse       Adm-clerical        Husband  White    Male   \n",
       "42656       Never-married                  ?      Own-child  Black  Female   \n",
       "280    Married-civ-spouse       Craft-repair        Husband  White    Male   \n",
       "\n",
       "       capital-gain  capital-loss  hours-per-week native-country income  \n",
       "37886             0             0              50  United-States   >50K  \n",
       "24949             0             0              40  United-States   >50K  \n",
       "24629          4386             0              40  United-States   >50K  \n",
       "25783             0             0              25  United-States   >50K  \n",
       "34688             0             0              42  United-States  <=50K  \n",
       "...             ...           ...             ...            ...    ...  \n",
       "16040             0             0              50  United-States   >50K  \n",
       "5882              0             0              40  United-States   >50K  \n",
       "20602             0             0              40  United-States  <=50K  \n",
       "42656             0             0              40  United-States  <=50K  \n",
       "280               0             0              40  United-States   >50K  \n",
       "\n",
       "[43957 rows x 15 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data = transformer.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06621756,  0.        ,  0.        , ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [-0.00379732,  0.        ,  0.        , ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [-0.00945845,  0.        ,  1.        , ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       ...,\n",
       "       [ 0.34627709,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       [ 0.20624733,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       [ 0.05213279,  0.        ,  0.        , ...,  0.        ,\n",
       "         1.        ,  0.        ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43957, 164)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformer.inverse_transform(transformed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_info = transformer.output_info_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[SpanInfo(dim=1, activation_fn='tanh'),\n",
       "  SpanInfo(dim=10, activation_fn='softmax')],\n",
       " [SpanInfo(dim=9, activation_fn='softmax')],\n",
       " [SpanInfo(dim=1, activation_fn='tanh'),\n",
       "  SpanInfo(dim=9, activation_fn='softmax')],\n",
       " [SpanInfo(dim=16, activation_fn='softmax')],\n",
       " [SpanInfo(dim=1, activation_fn='tanh'),\n",
       "  SpanInfo(dim=10, activation_fn='softmax')],\n",
       " [SpanInfo(dim=7, activation_fn='softmax')],\n",
       " [SpanInfo(dim=15, activation_fn='softmax')],\n",
       " [SpanInfo(dim=6, activation_fn='softmax')],\n",
       " [SpanInfo(dim=5, activation_fn='softmax')],\n",
       " [SpanInfo(dim=2, activation_fn='softmax')],\n",
       " [SpanInfo(dim=1, activation_fn='tanh'),\n",
       "  SpanInfo(dim=6, activation_fn='softmax')],\n",
       " [SpanInfo(dim=1, activation_fn='tanh'),\n",
       "  SpanInfo(dim=9, activation_fn='softmax')],\n",
       " [SpanInfo(dim=1, activation_fn='tanh'),\n",
       "  SpanInfo(dim=10, activation_fn='softmax')],\n",
       " [SpanInfo(dim=42, activation_fn='softmax')],\n",
       " [SpanInfo(dim=2, activation_fn='softmax')]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_sampler import DataSampler\n",
    "sampler = DataSampler(data, output_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec, mask, idx, opt1prime = sampler.sample_condvec_train(batch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 15)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14,  8,  5,  2,  2])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#id of sampled column\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 4, 6])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#id of chosen category in the sampled column\n",
    "opt1prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def cross_entropy_conditional_loss(data, convec, mask, output_info):\n",
    "    \"\"\"\n",
    "    Used to compute the conditional loss for ensuring the generator produces the desired category as specified by the conditional vector\n",
    "    Inputs:\n",
    "    1) data -> raw data synthesized by the generator \n",
    "    2) output_info -> column informtion corresponding to the data transformer\n",
    "    3) convec -> conditional vectors used to synthesize a batch of data\n",
    "    4) mask -> a matrix to identify chosen one-hot-encodings across the batch\n",
    "    Outputs:\n",
    "    1) loss -> conditional loss corresponding to the generated batch \n",
    "    \"\"\"\n",
    "    tmp_loss = []\n",
    "    st = 0\n",
    "    st_c = 0\n",
    "    output_info_flat = [elem for sublist in output_info for elem in sublist]\n",
    "    for column_info in output_info_flat:\n",
    "        if column_info.activation_fn == 'tanh':\n",
    "            st += column_info.dim\n",
    "            continue\n",
    "        elif column_info.activation_fn == 'softmax':\n",
    "            ed = st + column_info.dim\n",
    "            ed_c = st_c + column_info.dim\n",
    "            logits = data[:, st:ed]\n",
    "            labels = convec[:, st_c:ed_c]\n",
    "            tmp = tf.nn.softmax_cross_entropy_with_logits(labels, logits)\n",
    "            tmp_loss.append(tmp)\n",
    "            st = ed\n",
    "            st_c = ed_c\n",
    "    tmp_loss = tf.stack(tmp_loss, axis=1)\n",
    "    loss = tf.reduce_mean(tmp_loss * mask)  \n",
    "    return loss      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 20:23:46.127085: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-05-08 20:23:46.129904: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2023-05-08 20:23:46.158494: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:3b:00.0 name: Quadro RTX 8000 computeCapability: 7.5\n",
      "coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 47.45GiB deviceMemoryBandwidth: 625.94GiB/s\n",
      "2023-05-08 20:23:46.158850: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \n",
      "pciBusID: 0000:af:00.0 name: Quadro RTX 8000 computeCapability: 7.5\n",
      "coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 47.45GiB deviceMemoryBandwidth: 625.94GiB/s\n",
      "2023-05-08 20:23:46.158890: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-05-08 20:23:46.160502: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-05-08 20:23:46.160557: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-05-08 20:23:46.161762: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-05-08 20:23:46.161970: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-05-08 20:23:46.163142: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-05-08 20:23:46.163649: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-05-08 20:23:46.166174: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-05-08 20:23:46.166826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1\n",
      "2023-05-08 20:23:46.167384: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-08 20:23:46.168939: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-05-08 20:23:46.312218: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:3b:00.0 name: Quadro RTX 8000 computeCapability: 7.5\n",
      "coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 47.45GiB deviceMemoryBandwidth: 625.94GiB/s\n",
      "2023-05-08 20:23:46.312421: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \n",
      "pciBusID: 0000:af:00.0 name: Quadro RTX 8000 computeCapability: 7.5\n",
      "coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 47.45GiB deviceMemoryBandwidth: 625.94GiB/s\n",
      "2023-05-08 20:23:46.312458: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-05-08 20:23:46.312474: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-05-08 20:23:46.312483: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-05-08 20:23:46.312491: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-05-08 20:23:46.312499: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-05-08 20:23:46.312507: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-05-08 20:23:46.312515: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-05-08 20:23:46.312523: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-05-08 20:23:46.313027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1\n",
      "2023-05-08 20:23:46.313057: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-05-08 20:23:47.116311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-05-08 20:23:47.116347: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 1 \n",
      "2023-05-08 20:23:47.116352: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N Y \n",
      "2023-05-08 20:23:47.116354: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 1:   Y N \n",
      "2023-05-08 20:23:47.117583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 45371 MB memory) -> physical GPU (device: 0, name: Quadro RTX 8000, pci bus id: 0000:3b:00.0, compute capability: 7.5)\n",
      "2023-05-08 20:23:47.118310: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 531 MB memory) -> physical GPU (device: 1, name: Quadro RTX 8000, pci bus id: 0000:af:00.0, compute capability: 7.5)\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    cross_entropy_conditional_loss(data[:5], vec, mask, output_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = sampler.sample_data(5, idx, opt1prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 164)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_data() function in data_sampler.py\n",
    "# sample the transformed real data according to the conditional vector \n",
    "# data -> real transformed input data\n",
    "# _rid_by_cat_cols is a list of lists - >if we have 15 columns with one-hot code representation, we have 15 lists in _rid_by_cat_cols, each of these 15 lists are list of lists.\n",
    "# -> the number of these lists is equal to the length of one-hot encoded representation of that column. , the length of these lists are different, for example, the \n",
    "# i'th list contains the row index which the i'th element of the one-hot encoding representation is equal to 1. \n",
    "# in real data that is equal to \n",
    "data = transformed_data\n",
    "_rid_by_cat_cols = []\n",
    "n = len(data)\n",
    "st = 0\n",
    "output_info_flat = [elem for sublist in output_info for elem in sublist]\n",
    "for column_info in output_info_flat:\n",
    "    if column_info.activation_fn == 'tanh':\n",
    "        st += column_info.dim\n",
    "        continue\n",
    "    elif column_info.activation_fn == 'softmax':\n",
    "        ed = st + column_info.dim\n",
    "        rid_by_cat = []\n",
    "        for j in range(column_info.dim):\n",
    "            rid_by_cat.append(np.nonzero(data[:, st + j])[0])\n",
    "        _rid_by_cat_cols.append(rid_by_cat)\n",
    "        st = ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(_rid_by_cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "for c, o in zip(idx, opt1prime):\n",
    "    ids.append(np.random.choice(_rid_by_cat_cols[c][o]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7217, 5386, 3911, 14905, 35221]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.get_loc(target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[SpanInfo(dim=1, activation_fn='tanh'),\n",
       "  SpanInfo(dim=10, activation_fn='softmax')],\n",
       " [SpanInfo(dim=9, activation_fn='softmax')],\n",
       " [SpanInfo(dim=1, activation_fn='tanh'),\n",
       "  SpanInfo(dim=9, activation_fn='softmax')],\n",
       " [SpanInfo(dim=16, activation_fn='softmax')],\n",
       " [SpanInfo(dim=1, activation_fn='tanh'),\n",
       "  SpanInfo(dim=10, activation_fn='softmax')],\n",
       " [SpanInfo(dim=7, activation_fn='softmax')],\n",
       " [SpanInfo(dim=15, activation_fn='softmax')],\n",
       " [SpanInfo(dim=6, activation_fn='softmax')],\n",
       " [SpanInfo(dim=5, activation_fn='softmax')],\n",
       " [SpanInfo(dim=2, activation_fn='softmax')],\n",
       " [SpanInfo(dim=1, activation_fn='tanh'),\n",
       "  SpanInfo(dim=6, activation_fn='softmax')],\n",
       " [SpanInfo(dim=1, activation_fn='tanh'),\n",
       "  SpanInfo(dim=9, activation_fn='softmax')],\n",
       " [SpanInfo(dim=1, activation_fn='tanh'),\n",
       "  SpanInfo(dim=10, activation_fn='softmax')],\n",
       " [SpanInfo(dim=42, activation_fn='softmax')],\n",
       " [SpanInfo(dim=2, activation_fn='softmax')]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_col = 'relationship'\n",
    "df[target_col].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_st_ed(target_col, output_info):\n",
    "    \"\"\"\n",
    "    Used to obtain the start and ending positions of the target column as per the transformed data to be used by the classifier \n",
    "    Inputs:\n",
    "    1) target_col -> name of the target column used for machine learning tasks (binary/multi-classification) in the raw data \n",
    "    2) output_info -> column information corresponding to the data after applying the data transformer\n",
    "    Outputs:\n",
    "    1) starting (st) and ending (ed) positions of the target column as per the transformed data\n",
    "\n",
    "    \"\"\"\n",
    "    target_col_index = df.columns.get_loc(target_col)\n",
    "    st = 0\n",
    "    c = 0\n",
    "    length = 0\n",
    "    for info in output_info:\n",
    "        if c==target_col_index:\n",
    "            target_info = info\n",
    "            break\n",
    "        for item in info:\n",
    "            st += item.dim\n",
    "        c += 1\n",
    "    for item in target_info:\n",
    "        length += item.dim\n",
    "    ed = st + length \n",
    "    return(st, ed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_ed = get_st_ed(target_col, output_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the dimension of transformed data\n",
    "output_info = transformer.output_info_list\n",
    "input_dim = 0\n",
    "for info in output_info:\n",
    "    for item in info:\n",
    "        input_dim += item.dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dimension of input to classifier\n",
    "dim = input_dim - (st_ed[1] - st_ed[0])\n",
    "dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class Dense(tf.keras.layers.Layer):\n",
    "  def __init__(self, input_dim, output_dim):\n",
    "     super(Dense, self).__init__()\n",
    "     bound = 1/math.sqrt(input_dim)\n",
    "     w_init = tf.keras.initializers.RandomUniform(minval=-bound, maxval=bound)\n",
    "     self.w = tf.Variable(\n",
    "            initial_value=w_init(shape=(input_dim, output_dim), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "     b_init = tf.keras.initializers.RandomUniform(minval=-bound, maxval=bound)\n",
    "     self.b = tf.Variable(\n",
    "            initial_value=b_init(shape=(output_dim,), dtype=\"float32\"), trainable=True\n",
    "        )\n",
    "  def call(self,inputs):\n",
    "      return tf.matmul(inputs, self.w) + self.b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make classifier\n",
    "class_dim=(100, 200, 300, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 158)]             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               15900     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 300)               60300     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 400)               120400    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 2406      \n",
      "=================================================================\n",
      "Total params: 219,206\n",
      "Trainable params: 219,206\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp_classifier = tf.keras.layers.Input(shape=(dim,))\n",
    "input_dim = dim\n",
    "x = inp_classifier\n",
    "for output_dim in class_dim:\n",
    "    x = Dense(input_dim, output_dim)(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    input_dim = output_dim\n",
    "input_to_last_layer = output_dim\n",
    "# in case of binary classification the last layer outputs a single numeric value which is squashed to a probability with sigmoid\n",
    "if (st_ed[1] - st_ed[0]) == 2:\n",
    "    x = Dense(input_to_last_layer, 1)(x)\n",
    "    out = tf.keras.activations.sigmoid(x)\n",
    "# in case of multi-classs classification, the last layer outputs an array of numeric values associated to each class   \n",
    "else:\n",
    "    out = Dense(input_to_last_layer, st_ed[1] - st_ed[0])(x)\n",
    "classifier = tf.keras.models.Model(inp_classifier, out)\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctabgan import CTABGAN\n",
    "ctabgan = CTABGAN()\n",
    "discriminator, discriminator_rep, dside = ctabgan.make_discriminator(sampler, transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler.n_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtaining the desired height/width for converting tabular data records to square images for feeding it to discriminator network \t\t\n",
    "sides = [4, 8, 16, 24, 32]\n",
    "data_dim = transformer.output_dimensions\t\t\n",
    "# the discriminator takes the transformed training data concatenated by the corresponding conditional vectors as input\n",
    "col_size_d = data_dim + sampler.n_opt\n",
    "for i in sides:\n",
    "    if i * i >= col_size_d:\n",
    "        dside = i\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#discriminator \n",
    "num_channels = 64\n",
    "layer_dims = [(1, dside), (num_channels, dside // 2)]\n",
    "\n",
    "while layer_dims[-1][1] > 3 and len(layer_dims) < 4:\n",
    "    # the number of channels increases by a factor of 2 whereas the height/width decreases by the same factor with each layer\n",
    "    layer_dims.append((layer_dims[-1][0] * 2, layer_dims[-1][1] // 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 24), (64, 12), (128, 6), (256, 3)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_dims[-1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 24, 24, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 12, 12, 64)        1024      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 12, 12, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 6, 6, 128)         131072    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 3, 256)         524288    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 3, 3, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 1, 1, 1)           2305      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 660,483\n",
      "Trainable params: 659,587\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "inp = tf.keras.layers.Input(shape=(dside, dside, 1))\n",
    "x = inp\n",
    "for curr in layer_dims[1:]:\n",
    "    x = tf.keras.layers.Conv2D(filters=curr[0], kernel_size=4, strides=2, padding='same', use_bias=False)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "feature_rep = x\n",
    "x = tf.keras.layers.Conv2D(filters=1, kernel_size=layer_dims[-1][1], strides=1, padding='valid')(x)\n",
    "x_flat = tf.keras.layers.Flatten()(x)\n",
    "out = tf.keras.layers.Dense(1, activation='sigmoid')(x_flat)\n",
    "discriminator = tf.keras.models.Model(inp, out)\n",
    "discriminator_rep = tf.keras.models.Model(inp, feature_rep)\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_flat.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## input to generator\n",
    "batch_size = 5\n",
    "random_dim = 100\n",
    "# Sampling noise vectors using a standard normal distribution\n",
    "noisez = tf.random.normal([batch_size, random_dim])\n",
    "#sampling conditional vector\n",
    "vec, mask, idx, opt1prime = sampler.sample_condvec_train(batch_size)\n",
    "vec = tf.convert_to_tensor(vec, dtype=tf.float32)\n",
    "mask = tf.convert_to_tensor(mask, dtype=tf.float32)\n",
    "# Concatenating conditional vectors and converting resulting noise vectors into the image domain to be fed to the generator as input\n",
    "noisez = tf.concat([noisez, vec], axis=1)\n",
    "noisez = tf.reshape(noisez, [batch_size, 1, 1, random_dim + sampler.n_opt])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# obtaining the desired height/width for generating square images from the generator network that can be converted back to tabular domain \n",
    "data_dim = transformer.output_dimensions\t\t\n",
    "sides = [4, 8, 16, 24, 32]\n",
    "col_size_g = data_dim\n",
    "for i in sides:\n",
    "    if i * i >= col_size_g:\n",
    "        gside = i\n",
    "        break\n",
    "\n",
    "num_channels = 64\n",
    "#num_channels=\n",
    "# computing the dimensionality of hidden layers\n",
    "layer_dims = [(1, gside), (num_channels, gside // 2)]\n",
    "\n",
    "while layer_dims[-1][1] > 3 and len(layer_dims) < 4:\n",
    "    layer_dims.append((layer_dims[-1][0] * 2, layer_dims[-1][1] // 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 16), (64, 8), (128, 4), (256, 2)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 2)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " layer_dims[-1][0], layer_dims[-1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 128\n",
      "128 64\n",
      "64 1\n"
     ]
    }
   ],
   "source": [
    "for prev, curr in zip(reversed(layer_dims), reversed(layer_dims[:-1])):\n",
    "    print(prev[0], curr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = tf.keras.layers.Input(shape=(1,1, random_dim + sampler.n_opt))\n",
    "x = tf.keras.layers.Conv2DTranspose(filters=256, kernel_size=2, strides=1, padding='valid', use_bias=False)(inp)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.ReLU()(x)\n",
    "x = tf.keras.layers.Conv2DTranspose(filters=128, kernel_size=4, strides=2, padding='same', use_bias=True)(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.ReLU()(x)\n",
    "x = tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=4, strides=2, padding='same', use_bias=True)(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.ReLU()(x)\n",
    "out = tf.keras.layers.Conv2DTranspose(filters=1, kernel_size=4, strides=2, padding='same', use_bias=True)(x)\n",
    "generator = tf.keras.models.Model(inp, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 1, 1, 258)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 2, 2, 256)         264192    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 2, 2, 256)         1024      \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 4, 4, 128)         524416    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 8, 8, 64)          131136    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 16, 16, 1)         1025      \n",
      "=================================================================\n",
      "Total params: 922,561\n",
      "Trainable params: 921,665\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 1, 1, 258)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 2, 2, 256)         264192    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 2, 2, 256)         1024      \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTr (None, 4, 4, 128)         524416    \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTr (None, 8, 8, 64)          131136    \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "re_lu_5 (ReLU)               (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_7 (Conv2DTr (None, 16, 16, 1)         1025      \n",
      "=================================================================\n",
      "Total params: 922,561\n",
      "Trainable params: 921,665\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# other way of constructing generator\n",
    "inp = tf.keras.layers.Input(shape=(1,1, random_dim + sampler.n_opt))\n",
    "x = tf.keras.layers.Conv2DTranspose(filters=layer_dims[-1][0], kernel_size=layer_dims[-1][1], strides=1, padding='valid', use_bias=False)(inp)\n",
    "for curr in reversed(layer_dims[:-1]):\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    x = tf.keras.layers.Conv2DTranspose(filters=curr[0], kernel_size=4, strides=2, padding='same', use_bias=True)(x)\n",
    "generator = tf.keras.models.Model(inp, x)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 20:27:24.206811: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-05-08 20:27:24.423901: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    fake = generator(noisez)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([5, 16, 16, 1])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "faket = tf.reshape(fake, (-1, gside * gside))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([5, 256])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faket.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([5, 164])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## sample real data\n",
    "# Sampling real data according to the conditional vectors and shuffling it before feeding to discriminator to isolate conditional loss on generator\n",
    "perm = np.arange(batch_size)\n",
    "np.random.shuffle(perm)\n",
    "real = sampler.sample_data(batch_size, idx[perm], opt1prime[perm])\n",
    "real = tf.convert_to_tensor(real.astype('float32'))\n",
    "\n",
    "# Storing shuffled ordering of the conditional vectors\n",
    "vec_perm = tf.gather(vec, perm)\n",
    "real.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "def _gumbel_softmax(logits, tau=1.0, hard=False, dim=-1):\n",
    "        \"\"\"Samples from the Gumbel-Softmax distribution\n",
    "        :cite:`maddison2016concrete`, :cite:`jang2016categorical` and\n",
    "        optionally discretizes.\n",
    "        Parameters\n",
    "        ----------\n",
    "        logits: tf.Tensor\n",
    "            Un-normalized log probabilities.\n",
    "        tau: float, default=1.0\n",
    "            Non-negative scalar temperature.\n",
    "        hard: bool, default=False\n",
    "            If ``True``, the returned samples will be discretized as\n",
    "            one-hot vectors, but will be differentiated as soft samples.\n",
    "        dim: int, default=1\n",
    "            The dimension along which softmax will be computed.\n",
    "        Returns\n",
    "        -------\n",
    "        tf.Tensor\n",
    "            Sampled tensor of same shape as ``logits`` from the\n",
    "            Gumbel-Softmax distribution. If ``hard=True``, the returned samples\n",
    "            will be one-hot, otherwise they will be probability distributions\n",
    "            that sum to 1 across ``dim``.\n",
    "        \"\"\"\n",
    "\n",
    "        gumbel_dist = tfp.distributions.Gumbel(loc=0, scale=1)\n",
    "        gumbels = gumbel_dist.sample(tf.shape(logits))\n",
    "        gumbels = (logits + gumbels) / tau\n",
    "        output = tf.nn.softmax(gumbels, dim)\n",
    "\n",
    "        if hard:\n",
    "            index = tf.math.reduce_max(output, 1, keepdims=True)\n",
    "            output_hard = tf.cast(tf.equal(output, index), output.dtype)\n",
    "            output = tf.stop_gradient(output_hard - output) + output\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([5, 164])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## applying final activation on the generated data (i.e., tanh for numeric and gumbel-softmax for categorical)\n",
    "data = faket\n",
    "data_t = []\n",
    "st = 0\n",
    "for column_info in output_info:\n",
    "    for span_info in column_info:\n",
    "        if span_info.activation_fn == 'tanh':\n",
    "            ed = st + span_info.dim\n",
    "            data_t.append(tf.math.tanh(data[:, st:ed]))\n",
    "            st = ed\n",
    "        elif span_info.activation_fn == 'softmax':\n",
    "            ed = st + span_info.dim\n",
    "            transformed = _gumbel_softmax(data[:, st:ed], tau=0.2)\n",
    "            data_t.append(transformed)\n",
    "            st = ed\n",
    "        else:\n",
    "            raise ValueError(f'Unexpected activation function {span_info.activation_fn}.')\n",
    "\n",
    "fakeact = tf.concat(data_t, axis=1)\n",
    "fakeact.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the generated data is then concatenated with the corresponding condition vectors\n",
    "fake_cat = tf.concat([fakeact, vec], axis=1)\n",
    "# the real data is also similarly concatenated with corresponding conditional vectors   \n",
    "real_cat = tf.concat([real, vec_perm], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([5, 322]), TensorShape([5, 322]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "The Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "fake_cat.shape, real_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the real and synthetic data into the image domain for feeding it to the discriminator\n",
    "def transform(dside, data):\n",
    "    if dside * dside > len(data[0]):\n",
    "        # Tabular data records are padded with 0 to conform to square shaped images\n",
    "        padding = tf.zeros((len(data), dside * dside - len(data[0])), dtype=data.dtype)\n",
    "        data = tf.concat([data, padding], axis=1)\n",
    "\n",
    "    reshaped_data = tf.reshape(data, (-1, dside, dside, 1))\n",
    "    return reshaped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the real and synthetic data into the image domain for feeding it to the discriminator\n",
    "real_cat_d = transform(dside, real_cat)\n",
    "fake_cat_d = transform(dside, fake_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "     # computing the probability of the discriminator to correctly classify real samples hence y_real should ideally be close to 1\n",
    "     y_real = discriminator(real_cat_d)\n",
    "     # computing the probability of the discriminator to correctly classify fake samples hence y_fake should ideally be close to 0\n",
    "     y_fake = discriminator(fake_cat_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       "array([[0.49994826],\n",
       "       [0.5011007 ],\n",
       "       [0.5017541 ],\n",
       "       [0.49986437],\n",
       "       [0.50209063]], dtype=float32)>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we added a small value of 1e-4 to prevent taking the logarithm of zero, which could cause numerical instability\n",
    "y_real = y_real + 1e-4\n",
    "y_fake = y_fake + 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_D_real = tf.keras.losses.binary_crossentropy(tf.ones_like(y_real), y_real)\n",
    "loss_D_fake = tf.keras.losses.binary_crossentropy(tf.zeros_like(y_fake), y_fake)\n",
    "disc_loss = loss_D_real + loss_D_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample noise vector and conditional vector\n",
    "## input to generator(this time for training generator)\n",
    "batch_size = 5\n",
    "random_dim = 100\n",
    "# Sampling noise vectors using a standard normal distribution\n",
    "noisez = tf.random.normal([batch_size, random_dim])\n",
    "#sampling conditional vector\n",
    "vec, mask, idx, opt1prime = sampler.sample_condvec_train(batch_size)\n",
    "vec = tf.convert_to_tensor(vec, dtype=tf.float32)\n",
    "mask = tf.convert_to_tensor(mask, dtype=tf.float32)\n",
    "# Concatenating conditional vectors and converting resulting noise vectors into the image domain to be fed to the generator as input\n",
    "noisez = tf.concat([noisez, vec], axis=1)\n",
    "noisez = tf.reshape(noisez, [batch_size, 1, 1, random_dim + sampler.n_opt])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = generator(noisez)\n",
    "faket = tf.reshape(fake, (-1, gside * gside))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "## applying final activation on the generated data (i.e., tanh for numeric and gumbel-softmax for categorical)\n",
    "data = faket\n",
    "data_t = []\n",
    "st = 0\n",
    "for column_info in output_info:\n",
    "    for span_info in column_info:\n",
    "        if span_info.activation_fn == 'tanh':\n",
    "            ed = st + span_info.dim\n",
    "            data_t.append(tf.math.tanh(data[:, st:ed]))\n",
    "            st = ed\n",
    "        elif span_info.activation_fn == 'softmax':\n",
    "            ed = st + span_info.dim\n",
    "            transformed = _gumbel_softmax(data[:, st:ed], tau=0.2)\n",
    "            data_t.append(transformed)\n",
    "            st = ed\n",
    "        else:\n",
    "            raise ValueError(f'Unexpected activation function {span_info.activation_fn}.')\n",
    "\n",
    "fakeact = tf.concat(data_t, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the generated data is then concatenated with the corresponding condition vectors\n",
    "fake_cat = tf.concat([fakeact, vec], axis=1)\n",
    "fake_cat = transform(dside, fake_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_fake = discriminator(fake_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_fake = discriminator_rep(fake_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_cat = real_cat_d\n",
    "info_real = discriminator_rep(real_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.110063314>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# computing the conditional loss to ensure the generator generates data records with the chosen category as per the conditional vector\n",
    "cross_entropy = cross_entropy_conditional_loss(data, vec, mask, output_info)\n",
    "cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
       "array([0.69504464, 0.6899556 , 0.69013166, 0.69208086, 0.69000715],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_loss = tf.keras.losses.binary_crossentropy(tf.ones_like(y_fake), y_fake) #it is equivalent to -tf.math.log(y_fake)\n",
    "gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the loss to train the generator where we want y_fake to be close to 1 to fool the discriminator \n",
    "# and cross_entropy to be close to 0 to ensure generator's output matches the conditional vector\n",
    "gen_loss = tf.keras.losses.binary_crossentropy(tf.ones_like(y_fake), y_fake) + cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the information loss by comparing means and stds of real/fake feature representations extracted from discriminator's penultimate layer\n",
    "loss_mean = tf.norm(tf.math.reduce_mean(info_fake, axis=0) - tf.math.reduce_mean(info_real, axis=0), ord=1)\n",
    "loss_std = tf.norm(tf.math.reduce_std(info_fake, axis=0) - tf.math.reduce_std(info_real, axis=0), ord=1)\n",
    "loss_info = loss_mean + loss_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=3.9331427>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing gradients\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mainenv",
   "language": "python",
   "name": "mainenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
